{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark libraries\n",
    "import pyspark\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "# data wrangling\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use spark! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create local spark enviroment\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the three quarters from 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.csv('../data/data_Q1_2019/*.csv', header=True)\n",
    "df2 = spark.read.csv('../data/data_Q2_2019/*.csv', header=True)\n",
    "df3 = spark.read.csv('../data/data_Q3_2019/*.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n",
      "129\n",
      "129\n"
     ]
    }
   ],
   "source": [
    "for df in [df1,df2,df3]:\n",
    "    print(len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine 2019 dataframes together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2019 = df1.union(df2).union(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the 4 quarters from 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.csv('../data/data_Q1_2018/*.csv', header=True)\n",
    "df2 = spark.read.csv('../data/data_Q2_2018/*.csv', header=True)\n",
    "df3 = spark.read.csv('../data/data_Q3_2018/*.csv', header=True)\n",
    "df4 = spark.read.csv('../data/data_Q4_2018/*.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "109\n",
      "109\n",
      "129\n"
     ]
    }
   ],
   "source": [
    "for df in [df1,df2,df3,df4]:\n",
    "    print(len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the columns don't match, find the missing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = df1.columns\n",
    "list4 = df4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_traits_list = list(set(list4).difference(list1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['smart_170_normalized',\n",
       " 'smart_23_raw',\n",
       " 'smart_16_raw',\n",
       " 'smart_168_raw',\n",
       " 'smart_174_normalized',\n",
       " 'smart_231_raw',\n",
       " 'smart_232_normalized',\n",
       " 'smart_173_raw',\n",
       " 'smart_231_normalized',\n",
       " 'smart_218_raw',\n",
       " 'smart_173_normalized',\n",
       " 'smart_24_raw',\n",
       " 'smart_232_raw',\n",
       " 'smart_233_normalized',\n",
       " 'smart_218_normalized',\n",
       " 'smart_168_normalized',\n",
       " 'smart_233_raw',\n",
       " 'smart_24_normalized',\n",
       " 'smart_17_normalized',\n",
       " 'smart_174_raw',\n",
       " 'smart_23_normalized',\n",
       " 'smart_170_raw',\n",
       " 'smart_16_normalized',\n",
       " 'smart_17_raw']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_traits_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove missing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trait in remove_traits_list:\n",
    "    df2 = df2.drop(trait)\n",
    "    df3 = df3.drop(trait)\n",
    "    df4 = df4.drop(trait)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "105\n",
      "105\n",
      "105\n"
     ]
    }
   ],
   "source": [
    "for df in [df1,df2,df3,df4]:\n",
    "    print(len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine 2018 dataframes together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018 = df1.union(df2).union(df3).union(df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the four quarters from 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.csv('../data/data_Q1_2017/*.csv', header=True)\n",
    "df2 = spark.read.csv('../data/data_Q2_2017/*.csv', header=True)\n",
    "df3 = spark.read.csv('../data/data_Q3_2017/*.csv', header=True)\n",
    "df4 = spark.read.csv('../data/data_Q4_2017/*.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n",
      "95\n",
      "95\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "for df in [df1,df2,df3,df4]:\n",
    "    print(len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine the 2017 dataframes together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017 = df1.union(df2).union(df3).union(df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the four quarters from 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.csv('../data/data_Q1_2016/*.csv', header=True)\n",
    "df2 = spark.read.csv('../data/data_Q2_2016/*.csv', header=True)\n",
    "df3 = spark.read.csv('../data/data_Q3_2016/*.csv', header=True)\n",
    "df4 = spark.read.csv('../data/data_Q4_2016/*.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n",
      "95\n",
      "95\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "for df in [df1,df2,df3,df4]:\n",
    "    print(len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine the 2016 dataframes together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2016 = df1.union(df2).union(df3).union(df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract max values from top five SMART attributes and convert to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_pandas(df):\n",
    "    df = df.groupby('serial_number', 'model', 'capacity_bytes'\n",
    "              ).agg(\n",
    "                max('failure'), max('smart_9_raw'), max('smart_5_raw'), \n",
    "                max('smart_187_raw') , max('smart_197_raw'), max('smart_198_raw')\n",
    "                )\n",
    "    df = df.select(\"*\").toPandas()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save dfs as csv for easy access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = data_to_pandas(df_2019)\n",
    "# df.to_csv(r'./hard_drives_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = data_to_pandas(df_2018)\n",
    "# df.to_csv(r'./hard_drives_2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = data_to_pandas(df_2017)\n",
    "# df.to_csv(r'./hard_drives_2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = data_to_pandas(df_2016)\n",
    "# df.to_csv(r'./hard_drives_2016.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
